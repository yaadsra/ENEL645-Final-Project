{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X3Uc7mOgn786"
      },
      "source": [
        "1. Load in Imports required to run our code:\n",
        "\n",
        "    - We decided to process our model through a Jupyter file so that we could run the models using out local GPU's. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7dnGjmH1nzzS"
      },
      "outputs": [],
      "source": [
        "# Necessary imports for the project:\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import os\n",
        "from PIL import Image\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Define transformers for augmenting our dataset and normalizing each image w.r.t. the ImageNet dataset:\n",
        "\n",
        "    - preprecessing information that was used for resizing, center crop, and normalization can be found here: \n",
        "      https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transforms to augment our dataset to make it larger:\n",
        "hz_transform = transforms.RandomHorizontalFlip(p=1.0)\n",
        "vt_transform = transforms.RandomVerticalFlip(p=1.0)\n",
        "rot_transform = transforms.RandomRotation(degrees=10)\n",
        "jitter_transform = transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)\n",
        "gauss_transform = transforms.GaussianBlur(kernel_size=3)\n",
        "sharp_transform = transforms.RandomAdjustSharpness(sharpness_factor=1.5)\n",
        "perspective_transform = transforms.RandomPerspective(distortion_scale=0.3, p=1.0)\n",
        "\n",
        "# Define a transformer that will be applied on each image for preprocessing w.r.t. imageNet parameters:\n",
        "preprocess_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256), interpolation=InterpolationMode.BILINEAR),    # Pad images when necessary (reshape to square)\n",
        "    transforms.CenterCrop(224),                                                 # Center crop size to 224x224 (to match ImageNet)\n",
        "    transforms.ToTensor(),                                                      # Convert to tensor for PyTorch processing\n",
        "\n",
        "    # Values used for normalization are the same as from the ImageNet dataset:\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Importing dataset and augmenting training data:\n",
        "    - Our dataset was downloaded off of Kaggle from the following link: https://www.kaggle.com/datasets/mdwaquarazam/agricultural-crops-image-classification\n",
        "    - Two sets of data will be used for training 2 models\n",
        "        - The first will use the original dataset with an 80%, 10%, 10% split for the training, validation ,and testing sets, respectively\n",
        "        - The second will augment the training data with several transformers to increase the data size to compare with results from the first model that uses the original data\n",
        "            - Information on each of the augmentation transformers can be found here: https://pytorch.org/vision/master/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py\n",
        "    - A transformer is applied to all images based on the recommended ImageNet normalization and preprocessing recommendations\n",
        "    - Finally, we create training, validation, and testing datasets for both model variations (original and augmented):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original dataset with no image augmentations for training:\n",
            "\n",
            "The following agricultural crops will be classified in this project:\n",
            "\n",
            "['gram', 'sugarcane', 'Tobacco-plant', 'Lemon', 'rice', 'Pearl_millet(bajra)', 'cotton', 'Cucumber', 'chilli', 'Cherry', 'cardamom', 'tea', 'jowar', 'Olive-tree', 'wheat', 'vigna-radiati(Mung)', 'coconut', 'Fox_nut(Makhana)', 'almond', 'clove', 'Coffee-plant', 'mustard-oil', 'jute', 'banana', 'soyabean', 'papaya', 'pineapple', 'tomato', 'sunflower', 'maize'] \n",
            "\n",
            "Category:  gram :\t\t Training samples =  20 , Validation samples =  2 , Testing samples =  3\n",
            "Category:  sugarcane :\t\t Training samples =  20 , Validation samples =  2 , Testing samples =  3\n",
            "Category:  Tobacco-plant :\t\t Training samples =  26 , Validation samples =  3 , Testing samples =  4\n",
            "Category:  Lemon :\t\t Training samples =  22 , Validation samples =  3 , Testing samples =  3\n",
            "Category:  rice :\t\t Training samples =  23 , Validation samples =  3 , Testing samples =  3\n",
            "Category:  Pearl_millet(bajra) :\t\t Training samples =  31 , Validation samples =  4 , Testing samples =  4\n",
            "Category:  cotton :\t\t Training samples =  25 , Validation samples =  3 , Testing samples =  4\n",
            "Category:  Cucumber :\t\t Training samples =  24 , Validation samples =  3 , Testing samples =  4\n",
            "Category:  chilli :\t\t Training samples =  18 , Validation samples =  2 , Testing samples =  3\n",
            "Category:  Cherry :\t\t Training samples =  25 , Validation samples =  3 , Testing samples =  4\n",
            "Category:  cardamom :\t\t Training samples =  17 , Validation samples =  2 , Testing samples =  3\n",
            "Category:  tea :\t\t Training samples =  18 , Validation samples =  2 , Testing samples =  3\n",
            "Category:  jowar :\t\t Training samples =  24 , Validation samples =  3 , Testing samples =  3\n",
            "Category:  Olive-tree :\t\t Training samples =  24 , Validation samples =  3 , Testing samples =  3\n",
            "Category:  wheat :\t\t Training samples =  24 , Validation samples =  3 , Testing samples =  4\n",
            "Category:  vigna-radiati(Mung) :\t\t Training samples =  21 , Validation samples =  3 , Testing samples =  3\n",
            "Category:  coconut :\t\t Training samples =  20 , Validation samples =  2 , Testing samples =  3\n",
            "Category:  Fox_nut(Makhana) :\t\t Training samples =  18 , Validation samples =  2 , Testing samples =  3\n",
            "Category:  almond :\t\t Training samples =  16 , Validation samples =  2 , Testing samples =  3\n",
            "Category:  clove :\t\t Training samples =  24 , Validation samples =  3 , Testing samples =  3\n",
            "Category:  Coffee-plant :\t\t Training samples =  23 , Validation samples =  3 , Testing samples =  3\n",
            "Category:  mustard-oil :\t\t Training samples =  22 , Validation samples =  3 , Testing samples =  3\n",
            "Category:  jute :\t\t Training samples =  18 , Validation samples =  2 , Testing samples =  3\n",
            "Category:  banana :\t\t Training samples =  24 , Validation samples =  3 , Testing samples =  4\n",
            "Category:  soyabean :\t\t Training samples =  24 , Validation samples =  3 , Testing samples =  3\n",
            "Category:  papaya :\t\t Training samples =  18 , Validation samples =  2 , Testing samples =  3\n",
            "Category:  pineapple :\t\t Training samples =  20 , Validation samples =  2 , Testing samples =  3\n",
            "Category:  tomato :\t\t Training samples =  20 , Validation samples =  3 , Testing samples =  3\n",
            "Category:  sunflower :\t\t Training samples =  19 , Validation samples =  2 , Testing samples =  3\n",
            "Category:  maize :\t\t Training samples =  24 , Validation samples =  3 , Testing samples =  4\n",
            "\n",
            "The total number of training samples is: \t 652\n",
            "The total number of validation samples is: \t 79\n",
            "The total number of testing samples is: \t 98\n"
          ]
        }
      ],
      "source": [
        "#-------------------------------------------------------------------------------------------------\n",
        "# Import the 'Agricultural-crops' images from a local folder.\n",
        "# Split images from each classification label into individual testing, training, and validation sets.\n",
        "# Combine these individual sets into single testing, training, and validation sets. \n",
        "#-------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Set the folder path for image dataset:\n",
        "image_folder_1 = \"Agricultural-crops\"\n",
        "\n",
        "# Get a list of the different agricultural crop classifications:\n",
        "classifications_list_1 = [item for item in os.listdir(image_folder_1) if os.path.isdir(os.path.join(image_folder_1, item))]\n",
        "\n",
        "# Output the list of crops that will be classified in this project:\n",
        "print(\"Original dataset with no image augmentations for training:\\n\")\n",
        "print(\"The following agricultural crops will be classified in this project:\\n\")\n",
        "print(classifications_list_1,\"\\n\")\n",
        "\n",
        "# Initialize lists to store final combined training, validation, and testing sets:\n",
        "combined_X_train_1 = []\n",
        "combined_y_train_1 = []\n",
        "\n",
        "combined_X_validation_1 = []\n",
        "combined_y_validation_1 = []\n",
        "\n",
        "combined_X_test_1 = []\n",
        "combined_y_test_1 = []\n",
        "\n",
        "# Iterate through all sub-folders in the \"Agricultural-crops\" folder:\n",
        "for category_index_1, category_1 in enumerate(classifications_list_1):\n",
        "\n",
        "    # Create path for the current sub-folder and store a list of the image file names in that folder: \n",
        "    curr_category_folder_1 = os.path.join(image_folder_1, category_1)\n",
        "    curr_image_files_1 = os.listdir(curr_category_folder_1)\n",
        "\n",
        "     # Initialize lists for storing image paths and the classification label associated to that path:\n",
        "    curr_images_1 = []\n",
        "    curr_classification_labels_1 = []\n",
        "\n",
        "    for curr_image_name_1 in curr_image_files_1:\n",
        "          \n",
        "        # Create the full path for the current image: \n",
        "        curr_image_path_1 = os.path.join(curr_category_folder_1, curr_image_name_1)\n",
        "\n",
        "        # load in the image:\n",
        "        curr_image_1 = Image.open(curr_image_path_1).convert('RGB')\n",
        "\n",
        "        # Apply the pre-processing transformation to original image, append it to the 'curr_images' list, and append the classification label for that image:\n",
        "        curr_image_1 = preprocess_transform(curr_image_1)\n",
        "        curr_images_1.append(curr_image_1)\n",
        "        curr_classification_labels_1.append(category_index_1)      \n",
        "\n",
        "    # Split the data for the current category into 80% training and the remaining 20% to be left over for validation and testing:\n",
        "    curr_X_train_1, curr_X_temp_1, curr_y_train_1, curr_y_temp_1 = train_test_split(curr_images_1, curr_classification_labels_1, test_size=0.2, random_state=42)\n",
        "    curr_X_validation_1, curr_X_test_1, curr_y_validation_1, curr_y_test_1 = train_test_split(curr_X_temp_1, curr_y_temp_1, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Extend the split data for the current category to the comined training, validation, and testing sets:\n",
        "    combined_X_train_1.extend(curr_X_train_1)\n",
        "    combined_y_train_1.extend(curr_y_train_1)\n",
        "\n",
        "    combined_X_validation_1.extend(curr_X_validation_1)\n",
        "    combined_y_validation_1.extend(curr_y_validation_1)\n",
        "\n",
        "    combined_X_test_1.extend(curr_X_test_1)\n",
        "    combined_y_test_1.extend(curr_y_test_1)\n",
        "\n",
        "    # Print a summary for the current category:\n",
        "    print(\"Category: \", category_1, \":\\t\\t Training samples = \", len(curr_X_train_1), \", Validation samples = \", len(curr_X_validation_1), \", Testing samples = \", len(curr_X_test_1))\n",
        "\n",
        "# Print samples in each final set:\n",
        "print(\"\\nThe total number of training samples is: \\t\", len(combined_X_train_1))\n",
        "print(\"The total number of validation samples is: \\t\", len(combined_X_validation_1))\n",
        "print(\"The total number of testing samples is: \\t\", len(combined_X_test_1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modified dataset with 7 augmentations per image in the training set:\n",
            "\n",
            "The following agricultural crops will be classified in this project:\n",
            "\n",
            "['gram', 'sugarcane', 'Tobacco-plant', 'Lemon', 'rice', 'Pearl_millet(bajra)', 'cotton', 'Cucumber', 'chilli', 'Cherry', 'cardamom', 'tea', 'jowar', 'Olive-tree', 'wheat', 'vigna-radiati(Mung)', 'coconut', 'Fox_nut(Makhana)', 'almond', 'clove', 'Coffee-plant', 'mustard-oil', 'jute', 'banana', 'soyabean', 'papaya', 'pineapple', 'tomato', 'sunflower', 'maize'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#-------------------------------------------------------------------------------------------------\n",
        "# Import the 'Agricultural-crops' images from a local folder.\n",
        "# Split images from each classification label into individual testing, training, and validation sets.\n",
        "# Apply data augmentations to increase size of dataset and apply preprocessing transformations.\n",
        "# Combine these individual sets into single testing, training, and validation sets. \n",
        "#-------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Set the folder path for image dataset:\n",
        "image_folder_2 = \"Agricultural-crops\"\n",
        "\n",
        "# Get a list of the different agricultural crop classifications:\n",
        "classifications_list_2 = [item for item in os.listdir(image_folder_2) if os.path.isdir(os.path.join(image_folder_2, item))]\n",
        "\n",
        "# Output the list of crops that will be classified in this project:\n",
        "print(\"Modified dataset with 7 augmentations per image in the training set:\\n\")\n",
        "print(\"The following agricultural crops will be classified in this project:\\n\")\n",
        "print(classifications_list_2,\"\\n\")\n",
        "\n",
        "# Initialize lists to store final combined training, validation, and testing sets:\n",
        "combined_X_train_2 = []\n",
        "combined_y_train_2 = []\n",
        "\n",
        "combined_X_validation_2 = []\n",
        "combined_y_validation_2 = []\n",
        "\n",
        "combined_X_test_2 = []\n",
        "combined_y_test_2 = []\n",
        "\n",
        "# Iterate through all sub-folders in the \"Agricultural-crops\" folder:\n",
        "for category_index_2, category_2 in enumerate(classifications_list_2):\n",
        "\n",
        "    # Create path for the current sub-folder and store a list of the image file names in that folder: \n",
        "    curr_category_folder_2 = os.path.join(image_folder_2, category_2)\n",
        "    curr_image_files_2 = os.listdir(curr_category_folder_2)\n",
        "\n",
        "     # Initialize lists for storing image paths and the classification label associated to that path:\n",
        "    curr_images_2 = []\n",
        "    curr_classification_labels_2 = []\n",
        "\n",
        "    for curr_image_name_2 in curr_image_files_2:\n",
        "          \n",
        "        # Create the full path for the current image: \n",
        "        curr_image_path_2 = os.path.join(curr_category_folder_2, curr_image_name_2)\n",
        "\n",
        "        # load in the image:\n",
        "        curr_image_2 = Image.open(curr_image_path_2).convert('RGB')\n",
        "\n",
        "        # Apply the pre-processing transformation to original image, append it to the 'curr_images' list, and append the classification label for that image:\n",
        "        curr_image_2 = preprocess_transform(curr_image_2)\n",
        "        curr_images_2.append(curr_image_2)\n",
        "        curr_classification_labels_2.append(category_index_2)  \n",
        "\n",
        "    # Split the data for the current category into 60% training (lower due to planned augmentation) and the remaining 40% to be left over for validation and testing:\n",
        "    curr_X_train_2, curr_X_temp_2, curr_y_train_2, curr_y_temp_2 = train_test_split(curr_images_2, curr_classification_labels_2, test_size=0.4, random_state=42)\n",
        "    curr_X_validation_2, curr_X_test_2, curr_y_validation_2, curr_y_test_2 = train_test_split(curr_X_temp_2, curr_y_temp_2, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Augment ONLY the training data:\n",
        "    aug_X_train_2 = []\n",
        "    aug_y_train_2 = []\n",
        "\n",
        "    for curr_image_2, curr_label_2 in zip(curr_X_train_2, curr_y_train_2):\n",
        "        \n",
        "        # Append original training image to the augmented training set:\n",
        "        aug_X_train_2.append(curr_image_2)\n",
        "        aug_y_train_2.append(curr_label_2)\n",
        "\n",
        "        # Run the image through the augmentation transformer to make 7 new images:\n",
        "        curr_augmented_images_2 = [\n",
        "            hz_transform(curr_image_2),\n",
        "            vt_transform(curr_image_2),\n",
        "            rot_transform(curr_image_2),\n",
        "            jitter_transform(curr_image_2),\n",
        "            gauss_transform(curr_image_2),\n",
        "            sharp_transform(curr_image_2),\n",
        "            perspective_transform(curr_image_2)\n",
        "        ]\n",
        "\n",
        "        # Apply the pre-processing transformation to all augmented training images, append them to the 'curr_images', and append the classification label for each image:\n",
        "        for curr_augmented_image_2 in curr_augmented_images_2:\n",
        "            curr_augmented_image_2 = preprocess_transform(transforms.ToPILImage()(curr_augmented_image_2))\n",
        "            aug_X_train_2.append(curr_augmented_image_2)\n",
        "            aug_y_train_2.append(curr_label_2)\n",
        "\n",
        "\n",
        "    # Extend the split data for the current category to the comined training, validation, and testing sets:\n",
        "    combined_X_train_2.extend(aug_X_train_2)\n",
        "    combined_y_train_2.extend(aug_y_train_2)\n",
        "\n",
        "    combined_X_validation_2.extend(curr_X_validation_2)\n",
        "    combined_y_validation_2.extend(curr_y_validation_2)\n",
        "\n",
        "    combined_X_test_2.extend(curr_X_test_2)\n",
        "    combined_y_test_2.extend(curr_y_test_2)\n",
        "\n",
        "    # Print a summary for the current category:\n",
        "    print(\"Category: \", category_2, \":\\t\\t Training samples = \", len(aug_X_train_2), \", Validation samples = \", len(curr_X_validation_2), \", Testing samples = \", len(curr_X_test_2))\n",
        "\n",
        "# Print samples in each final set:\n",
        "print(\"\\nThe total number of training samples is: \\t\", len(combined_X_train_2))\n",
        "print(\"The total number of validation samples is: \\t\", len(combined_X_validation_2))\n",
        "print(\"The total number of testing samples is: \\t\", len(combined_X_test_2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Visualize augmented data before running model if desired: (comment this out if you do not want to see dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot each image in curr_images individually:\n",
        "for index_2, image_2 in enumerate(curr_images_2):\n",
        "\n",
        "    # Set desired figure size:\n",
        "    plt.figure(figsize=(2, 2))\n",
        "\n",
        "    # Rearrange dimensions from tensor format so that matplotlib can read image correctly:\n",
        "    plt.imshow(image_2.permute(1, 2, 0))  \n",
        "\n",
        "    # Set image title based on current image number:\n",
        "    plt.title(f\"Image {index_2 + 1}\")\n",
        "\n",
        "    # Plot image with axis off:\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Convert data and labels into Tensors and then create dataloader instances for pytorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For first Iteration of dataset (No augmentation)\n",
        "\n",
        "# Convert training, validation, and testing data into pytorch tensors:\n",
        "X_train_tensor_1 = torch.stack(combined_X_train_1)\n",
        "y_train_tensor_1 = torch.tensor(combined_y_train_1)\n",
        "\n",
        "X_validation_tensor_1 = torch.stack(combined_X_validation_1)\n",
        "y_validation_tensor_1 = torch.tensor(combined_y_validation_1)\n",
        "\n",
        "X_test_tensor_1 = torch.stack(combined_X_test_1)\n",
        "y_test_tensor_1 = torch.tensor(combined_y_test_1)\n",
        "\n",
        "# Build pytorch tensor dataset objects:\n",
        "training_dataset_1 = TensorDataset(X_train_tensor_1, y_train_tensor_1)\n",
        "validation_dataset_1 = TensorDataset(X_validation_tensor_1, y_validation_tensor_1)\n",
        "testing_dataset_1 = TensorDataset(X_test_tensor_1, y_test_tensor_1)\n",
        "\n",
        "# Create dataLoader objects which batch (size = 64) and shuffle the training data:\n",
        "train_loader_1 = DataLoader(training_dataset_1, batch_size=32, shuffle=True)\n",
        "val_loader_1 = DataLoader(validation_dataset_1, batch_size=32, shuffle=False)\n",
        "test_loader_1 = DataLoader(testing_dataset_1, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For second Iteration of dataset (Augmentation)\n",
        "\n",
        "# Convert training, validation, and testing data into pytorch tensors:\n",
        "X_train_tensor_2 = torch.stack(combined_X_train_2)\n",
        "y_train_tensor_2 = torch.tensor(combined_y_train_2)\n",
        "\n",
        "X_validation_tensor_2 = torch.stack(combined_X_validation_2)\n",
        "y_validation_tensor_2 = torch.tensor(combined_y_validation_2)\n",
        "\n",
        "X_test_tensor_2 = torch.stack(combined_X_test_2)\n",
        "y_test_tensor_2 = torch.tensor(combined_y_test_2)\n",
        "\n",
        "# Build pytorch tensor dataset objects:\n",
        "training_dataset_2 = TensorDataset(X_train_tensor_2, y_train_tensor_2)\n",
        "validation_dataset_2 = TensorDataset(X_validation_tensor_2, y_validation_tensor_2)\n",
        "testing_dataset_2 = TensorDataset(X_test_tensor_2, y_test_tensor_2)\n",
        "\n",
        "# Create dataLoader objects which batch (size = 64) and shuffle the training data:\n",
        "train_loader_2 = DataLoader(training_dataset_2, batch_size=32, shuffle=True)\n",
        "val_loader_2 = DataLoader(validation_dataset_2, batch_size=32, shuffle=False)\n",
        "test_loader_2 = DataLoader(testing_dataset_2, batch_size=32, shuffle=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oE5Uw0mcpWvc"
      },
      "source": [
        "6. Model Definitions\n",
        "\n",
        "    - We decided to apply transfer learning using the pre-trained ResNet18 model (trained on ImageNet dataset)\n",
        "    - Initial weights are loaded in and frozen\n",
        "    - The final linear layer is applied where these weight are adjusted per epoch\n",
        "    - GPU will be used if available on computer running the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSVLZHjqo_mN",
        "outputId": "ce01448e-b5e5-4c85-96d6-f5f09dda8b7b"
      },
      "outputs": [],
      "source": [
        "# For first Iteration of dataset (No augmentation)\n",
        "\n",
        "# Instantiate the Resnet18 model using standard weights (did not implement freezing of layers):\n",
        "model_1 = torchvision.models.resnet50(weights=True)\n",
        "\n",
        "# Freeze Resnet50 layers:\n",
        "for parameter_1 in model_1.parameters():\n",
        "    parameter_1.requires_grad = False\n",
        "\n",
        "num_ftrs_1 = model_1.fc.in_features   # Define features to use from Resnet18\n",
        "num_classes_1 = 30                  # Define number of unique classification classes\n",
        "\n",
        "# Adjust the final fully connected layer based on the number of classes:\n",
        "model_1.fc = nn.Linear(num_ftrs_1, num_classes_1)\n",
        "\n",
        "# Check if a GPU is available and move the model to GPU if possible:\n",
        "device_1 = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device used:\", device_1)\n",
        "model_1 = model_1.to(device_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For second Iteration of dataset (Augmentation)\n",
        "\n",
        "# Instantiate the Resnet50 model using standard weights (did not implement freezing of layers):\n",
        "model_2 = torchvision.models.resnet50(weights=True)\n",
        "\n",
        "# Freeze Resnet50 layers:\n",
        "for parameter_2 in model_2.parameters():\n",
        "    parameter_2.requires_grad = False\n",
        "\n",
        "num_ftrs_2 = model_2.fc.in_features   # Define features to use from Resnet18\n",
        "num_classes_2 = 30                  # Define number of unique classification classes\n",
        "\n",
        "# Adjust the final fully connected layer based on the number of classes:\n",
        "model_2.fc = nn.Linear(num_ftrs_2, num_classes_2)\n",
        "\n",
        "# Check if a GPU is available and move the model to GPU if possible:\n",
        "device_2 = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device used:\", device_2)\n",
        "model_2 = model_2.to(device_2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zT6q-ygJpgWx"
      },
      "source": [
        "7. Loss Function and Optimizer\n",
        "\n",
        "    - Below, we define out loss function criteria (Cross Entropy) and optimizer (Stochastic Gradient Descent) that are used for learning. \n",
        "    - We played with the learning rate and momentum parameters in the optimizer, which resulted in the following results from initial tests:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQH6CXuio_eP"
      },
      "outputs": [],
      "source": [
        "# For first Iteration of dataset (No augmentation)\n",
        "\n",
        "# We use CrossEntropy loss function and the SGD optimizer:\n",
        "criterion_1 = nn.CrossEntropyLoss()\n",
        "optimizer_1 = optim.SGD(model_1.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For second Iteration of dataset (Augmentation)\n",
        "\n",
        "# We use CrossEntropy loss function and the SGD optimizer:\n",
        "criterion_2 = nn.CrossEntropyLoss()\n",
        "optimizer_2 = optim.SGD(model_2.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qt-BD5THpgxG"
      },
      "source": [
        "8. Training Loops:\n",
        "\n",
        "    - We chose to implement our training loops with validation occuring in each epoch\n",
        "    - When the validation loss has stopped decreasing onver 3 epochs, the training is terminated\n",
        "    - the best epoch model is then stored to be used for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr9LsRWqY9yM",
        "outputId": "f49207fe-eedd-4366-c8dd-6b414b648777"
      },
      "outputs": [],
      "source": [
        "# For first Iteration of dataset (No augmentation)\n",
        "print(\"Training results using the raw dataset (No augmentation):\\n\")\n",
        "\n",
        "# Initialize stopping parameters:\n",
        "no_improvement_count_1 = 0 \n",
        "best_val_loss_1 = 100000      # Initialize loss to be larger than first expected by a very large number\n",
        "max_num_epochs_1 = 60         # Code will stop if 60 epochs are reached\n",
        "patience_1 = 3                # Code will stop if validation loss hasnt decreased for 3 epochs\n",
        "\n",
        "# Initialize storage for tracking best epoch:\n",
        "best_epoch_1 = 0\n",
        "best_epoch_state_1 = None\n",
        "   \n",
        "\n",
        "for epoch_1 in range(max_num_epochs_1):\n",
        "\n",
        "    # Set model to training mode:\n",
        "    model_1.train()\n",
        "\n",
        "    # Initialize tracking parameters:\n",
        "    running_loss_1 = 0.0\n",
        "    correct_1 = 0\n",
        "    total_1 = 0\n",
        "\n",
        "  \n",
        "\n",
        "    # Training phase:\n",
        "    for images_1, labels_1 in train_loader_1:\n",
        "        images_1, labels_1 = images_1.to(device_1), labels_1.to(device_1)\n",
        "\n",
        "        # Clear gradients from previous epoch:\n",
        "        optimizer_1.zero_grad()\n",
        "\n",
        "        # Implement forward pass:\n",
        "        outputs_1 = model_1(images_1)\n",
        "        loss_1 = criterion_1(outputs_1, labels_1)\n",
        "\n",
        "        # Implement backward pass:\n",
        "        loss_1.backward()\n",
        "        optimizer_1.step()\n",
        "        running_loss_1 += loss_1.item()\n",
        "\n",
        "        # Convert outputs to predicted class labels:\n",
        "        __1, predicted_1 = torch.max(outputs_1.data, 1)\n",
        "\n",
        "        # Update count of processed labels and correctly predicted labels:\n",
        "        total_1 += labels_1.size(0)\n",
        "        correct_1 += (predicted_1 == labels_1).sum().item()\n",
        "\n",
        "    # Calculate training loss and accuracy for current epoch:\n",
        "    train_loss_1 = running_loss_1 / len(train_loader_1)\n",
        "    train_acc_1 = 100 * correct_1 / total_1\n",
        "\n",
        "    # Validation phase:\n",
        "\n",
        "    # Set the model to evaluation mode:\n",
        "    model_1.eval()\n",
        "\n",
        "    # Initialize tracking parameters:\n",
        "    val_running_loss_1 = 0.0\n",
        "    val_correct_1 = 0\n",
        "    val_total_1 = 0\n",
        "\n",
        "    # Iterate through batches without tracking gradients:\n",
        "    with torch.no_grad():\n",
        "        for images_1, labels_1 in val_loader_1:\n",
        "            images_1, labels_1 = images_1.to(device_1), labels_1.to(device_1)\n",
        "            outputs_1 = model_1(images_1)\n",
        "            loss_1 = criterion_1(outputs_1, labels_1)\n",
        "            val_running_loss_1 += loss_1.item()\n",
        "\n",
        "            # Convert outputs to predicted class labels:\n",
        "            __1, predicted_1 = torch.max(outputs_1.data, 1)\n",
        "\n",
        "            # Update count of processed labels and correctly predicted labels:\n",
        "            val_total_1 += labels_1.size(0)\n",
        "            val_correct_1 += (predicted_1 == labels_1).sum().item()\n",
        "\n",
        "    # Calculate validation loss and accuracy for current epoch:\n",
        "    val_loss_1 = val_running_loss_1 / len(val_loader_1)\n",
        "    val_acc_1 = 100 * val_correct_1 / val_total_1\n",
        "\n",
        "    # Add a stop condition based on validation loss:\n",
        "    if val_loss_1 < best_val_loss_1:\n",
        "        best_val_loss_1 = val_loss_1\n",
        "        no_improvement_count_1 = 0\n",
        "\n",
        "        # Store current epoch as 'best epoch' for running model on test data:\n",
        "        best_epoch_1 = epoch_1\n",
        "        best_epoch_state_1 = model_1.state_dict()\n",
        "    else:\n",
        "        no_improvement_count_1 = no_improvement_count_1 + 1\n",
        "\n",
        "    # Evaluate if stop condition has not decreased based on set patience:\n",
        "    if no_improvement_count_1 >= patience_1:\n",
        "        break\n",
        "\n",
        "    # Output current epoch losses and accuracies:\n",
        "    print(f'Epoch {epoch_1+1}, Train Loss: {train_loss_1:.4f}, Train Accuracy: {train_acc_1:.2f}%, Val Loss: {val_loss_1:.4f}, Val Accuracy: {val_acc_1:.2f}%')\n",
        "\n",
        "print(\"The best epoch for this model was epoch: \", best_epoch_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For second Iteration of dataset (Augmentation)\n",
        "print(\"Training results using the augmented dataset:\\n\")\n",
        "\n",
        "# Initialize stopping parameters:\n",
        "no_improvement_count_2 = 0 \n",
        "best_val_loss_2 = 100000      # Initialize loss to be larger than first expected by a very large number\n",
        "max_num_epochs_2 = 60         # Code will stop if 60 epochs are reached\n",
        "patience_2 = 3                # Code will stop if validation loss hasnt decreased for 3 epochs\n",
        "\n",
        "# Initialize storage for tracking best epoch:\n",
        "best_epoch_2 = 0\n",
        "best_epoch_state_2 = None\n",
        "   \n",
        "\n",
        "for epoch_2 in range(max_num_epochs_2):\n",
        "\n",
        "    # Set model to training mode:\n",
        "    model_2.train()\n",
        "\n",
        "    # Initialize tracking parameters:\n",
        "    running_loss_2 = 0.0\n",
        "    correct_2 = 0\n",
        "    total_2 = 0\n",
        "\n",
        "  \n",
        "\n",
        "    # Training phase:\n",
        "    for images_2, labels_2 in train_loader_2:\n",
        "        images_2, labels_2 = images_2.to(device_2), labels_2.to(device_2)\n",
        "\n",
        "        # Clear gradients from previous epoch:\n",
        "        optimizer_2.zero_grad()\n",
        "\n",
        "        # Implement forward pass:\n",
        "        outputs_2 = model_2(images_2)\n",
        "        loss_2 = criterion_2(outputs_2, labels_2)\n",
        "\n",
        "        # Implement backward pass:\n",
        "        loss_2.backward()\n",
        "        optimizer_2.step()\n",
        "        running_loss_2 += loss_2.item()\n",
        "\n",
        "        # Convert outputs to predicted class labels:\n",
        "        __2, predicted_2 = torch.max(outputs_2.data, 1)\n",
        "\n",
        "        # Update count of processed labels and correctly predicted labels:\n",
        "        total_2 += labels_2.size(0)\n",
        "        correct_2 += (predicted_2 == labels_2).sum().item()\n",
        "\n",
        "    # Calculate training loss and accuracy for current epoch:\n",
        "    train_loss_2 = running_loss_2 / len(train_loader_2)\n",
        "    train_acc_2 = 100 * correct_2 / total_2\n",
        "\n",
        "    # Validation phase:\n",
        "\n",
        "    # Set the model to evaluation mode:\n",
        "    model_2.eval()\n",
        "\n",
        "    # Initialize tracking parameters:\n",
        "    val_running_loss_2 = 0.0\n",
        "    val_correct_2 = 0\n",
        "    val_total_2 = 0\n",
        "\n",
        "    # Iterate through batches without tracking gradients:\n",
        "    with torch.no_grad():\n",
        "        for images_2, labels_2 in val_loader_2:\n",
        "            images_2, labels_2 = images_2.to(device_2), labels_2.to(device_2)\n",
        "            outputs_2 = model_2(images_2)\n",
        "            loss_2 = criterion_2(outputs_2, labels_2)\n",
        "            val_running_loss_2 += loss_2.item()\n",
        "\n",
        "            # Convert outputs to predicted class labels:\n",
        "            __2, predicted_2 = torch.max(outputs_2.data, 1)\n",
        "\n",
        "            # Update count of processed labels and correctly predicted labels:\n",
        "            val_total_2 += labels_2.size(0)\n",
        "            val_correct_2 += (predicted_2 == labels_2).sum().item()\n",
        "\n",
        "    # Calculate validation loss and accuracy for current epoch:\n",
        "    val_loss_2 = val_running_loss_2 / len(val_loader_2)\n",
        "    val_acc_2 = 100 * val_correct_2 / val_total_2\n",
        "\n",
        "    # Add a stop condition based on validation loss:\n",
        "    if val_loss_2 < best_val_loss_2:\n",
        "        best_val_loss_2 = val_loss_2\n",
        "        no_improvement_count_2 = 0\n",
        "\n",
        "        # Store current epoch as 'best epoch' for running model on test data:\n",
        "        best_epoch_2 = epoch_2\n",
        "        best_epoch_state_2 = model_2.state_dict()\n",
        "    else:\n",
        "        no_improvement_count_2 = no_improvement_count_2 + 1\n",
        "\n",
        "    # Evaluate if stop condition has not decreased based on set patience:\n",
        "    if no_improvement_count_2 >= patience_2:\n",
        "        break\n",
        "\n",
        "    # Output current epoch losses and accuracies:\n",
        "    print(f'Epoch {epoch_2+1}, Train Loss: {train_loss_2:.4f}, Train Accuracy: {train_acc_2:.2f}%, Val Loss: {val_loss_2:.4f}, Val Accuracy: {val_acc_2:.2f}%')\n",
        "\n",
        "print(\"The best epoch for this model was epoch: \", best_epoch_2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eGi-ad71qFie"
      },
      "source": [
        "9. Defining Model Testing functions:\n",
        "    - Weights from the best epoch during training are loaded in before evaluating testing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWNgtvtAo_LJ"
      },
      "outputs": [],
      "source": [
        "# For first Iteration of dataset (No augmentation)\n",
        "\n",
        "# Function defined to evaluate both models using our test dataset:\n",
        "def evaluate_model_1(model_1, data_loader_1, best_epoch_state_1):\n",
        "\n",
        "    # Set model to evaluate mode\n",
        "    model_1.eval()\n",
        "\n",
        "    # Load the best epoch state that was determined through training and validation stage:\n",
        "    model_1.load_state_dict(best_epoch_state_1)\n",
        "\n",
        "    # Initialize tracking parameters:\n",
        "    total_1 = 0\n",
        "    correct_1 = 0\n",
        "    all_preds_1 = []\n",
        "    all_labels_1 = []\n",
        "\n",
        "    # Iterate through batches without tracking gradients:\n",
        "    with torch.no_grad():\n",
        "        for images_1, labels_1 in data_loader_1:\n",
        "            images_1, labels_1 = images_1.to(device_1), labels_1.to(device_1)\n",
        "            outputs_1 = model_1(images_1)\n",
        "\n",
        "            # Convert outputs to predicted class labels:\n",
        "            __1, predicted_1 = torch.max(outputs_1, 1)\n",
        "\n",
        "            # Update count of processed labels and correctly predicted labels:\n",
        "            total_1 += labels_1.size(0)\n",
        "            correct_1 += (predicted_1 == labels_1).sum().item()\n",
        "\n",
        "            # Collect predictions and labels:\n",
        "            all_preds_1.extend(predicted_1.view(-1).cpu().numpy())\n",
        "            all_labels_1.extend(labels_1.view(-1).cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy and return results:\n",
        "    accuracy_1 = 100 * correct_1 / total_1\n",
        "    return accuracy_1, all_preds_1, all_labels_1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For second Iteration of dataset (Augmentation)\n",
        "\n",
        "# Function defined to evaluate both models using our test dataset:\n",
        "def evaluate_model_2(model_2, data_loader_2, best_epoch_state_2):\n",
        "\n",
        "    # Set model to evaluate mode\n",
        "    model_2.eval()\n",
        "\n",
        "    # Load the best epoch state that was determined through training and validation stage:\n",
        "    model_2.load_state_dict(best_epoch_state_2)\n",
        "\n",
        "    # Initialize tracking parameters:\n",
        "    total_2 = 0\n",
        "    correct_2 = 0\n",
        "    all_preds_2 = []\n",
        "    all_labels_2 = []\n",
        "\n",
        "    # Iterate through batches without tracking gradients:\n",
        "    with torch.no_grad():\n",
        "        for images_2, labels_2 in data_loader_2:\n",
        "            images_2, labels_2 = images_2.to(device_2), labels_2.to(device_2)\n",
        "            outputs_2 = model_2(images_2)\n",
        "\n",
        "            # Convert outputs to predicted class labels:\n",
        "            __2, predicted_2 = torch.max(outputs_2, 1)\n",
        "\n",
        "            # Update count of processed labels and correctly predicted labels:\n",
        "            total_2 += labels_2.size(0)\n",
        "            correct_2 += (predicted_2 == labels_2).sum().item()\n",
        "\n",
        "            # Collect predictions and labels:\n",
        "            all_preds_2.extend(predicted_2.view(-1).cpu().numpy())\n",
        "            all_labels_2.extend(labels_2.view(-1).cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy and return results:\n",
        "    accuracy_2 = 100 * correct_2 / total_2\n",
        "    return accuracy_2, all_preds_2, all_labels_2\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "10. Evaluation Metrics and Testing:\n",
        "\n",
        "    - Below is our blocks of code that evaluate the models using the testing data\n",
        "    - We focused on evaluating the model using metrics such as accuracy, precision, recall, and f-score\n",
        "    - We also presented the confusion matrix showing insight into how the results looked for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "_0wca-yipoWm",
        "outputId": "55508398-0543-4d24-a770-706afec126a5"
      },
      "outputs": [],
      "source": [
        "# For first Iteration of dataset (No augmentation)\n",
        "\n",
        "# Evaluate test dataset using function defined in block above:\n",
        "test_accuracy_1, test_preds_1, test_labels_1 = evaluate_model_1(model_1, test_loader_1, best_epoch_state_1)\n",
        "print(f'Test Accuracy: {test_accuracy_1:.2f}%')\n",
        "\n",
        "# Get class names:\n",
        "print(classifications_list_1)\n",
        "\n",
        "# Define and plot confusion Matrix:\n",
        "cm_1 = confusion_matrix(test_labels_1, test_preds_1)\n",
        "class_names_1 = classifications_list_1\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm_1, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names_1, yticklabels=class_names_1)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix using Original Training Dataset')\n",
        "plt.show()\n",
        "\n",
        "# Output classification Report:\n",
        "print(classification_report(test_labels_1, test_preds_1, target_names=class_names_1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For second Iteration of dataset (Augmentation)\n",
        "\n",
        "# Evaluate test dataset using function defined in block above:\n",
        "test_accuracy_2, test_preds_2, test_labels_2 = evaluate_model_2(model_2, test_loader_2, best_epoch_state_2)\n",
        "print(f'Test Accuracy: {test_accuracy_2:.2f}%')\n",
        "\n",
        "# Get class names:\n",
        "print(classifications_list_2)\n",
        "\n",
        "# Define and plot confusion Matrix:\n",
        "cm_2 = confusion_matrix(test_labels_2, test_preds_2)\n",
        "class_names_2 = classifications_list_2\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm_2, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names_2, yticklabels=class_names_2)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix using Augmented Training Dataset')\n",
        "plt.show()\n",
        "\n",
        "# Output classification Report:\n",
        "print(classification_report(test_labels_2, test_preds_2, target_names=class_names_2))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
